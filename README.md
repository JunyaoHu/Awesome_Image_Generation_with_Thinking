# üí°üéûÔ∏è Awesome_Image_Generation_with_Thinking

<div align="center">
  <img src="logo.png" alt="Logo" width="300">
  <h1 align="center">Image Generation with Thinking.</h1>
  
[![Awesome](https://awesome.re/badge.svg)](https://github.com/XiaoYee/Awesome_Efficient_LRM_Reasoning) 
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
![](https://img.shields.io/github/last-commit/zhaochen0110/Awesome_Think_With_Images?color=green) 

</div>

Welcome to the Awesome-Image-Generation-with-Thinking repository! This repository represents a comprehensive collection of research focused on empowering models to think during image generation. We delve into how these sophisticated models are evolving beyond mere pattern recognition, acquiring capabilities for intricate reasoning, nuanced understanding, and dynamic interaction by processing and interpreting visual information in cognitive-inspired ways.

This collection is for researchers, developers, and enthusiasts eager to explore the forefront of:
*   **Prompt-Based Innovation:** How LVLMs can guide visual understanding and generation.
*   **Supervised Fine-Tuning:** Training models with rich, contextual visual data.
*   **Reinforcement Learning:** Enabling agents to learn through visual interaction and feedback.


---

## üîî News

- [2025-06] We created this repository to maintain a paper list on Awesome-Think-With-Images. Contributions are welcome!

- [2025-05] We are excited to release **[OpenThinkIMG](https://github.com/OpenThinkIMG/OpenThinkIMG)**, the first dedicated end-to-end open-source framework designed to empower LVLMs to truly **think with images**! For ease of use, we've configured a Docker environment. We warmly invite the community to explore, use, and contribute.

---

## üìú Table of Contents

*   [‚úçÔ∏è Editing-based explicitly reflection](#-editing-based-explicitly-reflection)
*   [üóíÔ∏è Reasoning with prompt](#-reasoning-with-prompt)
*   [üèÜ RL for self-evolution](#-RL-for-self-evolution)
*   [üìö Benchmarks](#-benchmarks)

---

## üöÄ Editing-based explicitly reflection

Unlocking visual intelligence through the art and science of prompting. These methods explore how carefully crafted textual or visual cues can guide LVLMs to perform complex reasoning tasks with images, often without explicit task-specific training.

- [Visual programming: Compositional visual reasoning without training](https://arxiv.org/abs/2211.11559) (CVPR, 2023) <br>
[![GitHub stars](https://img.shields.io/github/stars/allenai/visprog)](https://github.com/allenai/visprog)
[![Website](https://img.shields.io/badge/Website-Visit-blue?style=flat-square)](https://prior.allenai.org/projects/visprog)


- [ViperGPT: Visual Inference via Python Execution for Reasoning](https://arxiv.org/abs/2303.08128) (ICCV, 2023) <br>
[![GitHub stars](https://img.shields.io/github/stars/cvlab-columbia/viper)](https://github.com/cvlab-columbia/viper)


- [From reflection to perfection: Scaling inference-time optimization for text-to-image diffusion models via reflection tuning](https://arxiv.org/abs/2504.16080) (Apr., 2025) <br>
[![GitHub stars](https://img.shields.io/github/stars/Diffusion-CoT/ReflectionFlow)](https://github.com/Diffusion-CoT/ReflectionFlow)
[![Website](https://img.shields.io/badge/Website-Visit-blue?style=flat-square)](https://diffusion-cot.github.io/reflection2perfection/)
[![Dataset](https://img.shields.io/badge/Dataset-Available-brightgreen?style=flat-square)](https://huggingface.co/collections/diffusion-cot/reflectionflow-release-6803e14352b1b13a16aeda44)


- [GoT: Unleashing reasoning capability of multimodal large language model for visual generation and editing](https://arxiv.org/abs/2503.10639) (Mar., 2025) <br>
[![GitHub stars](https://img.shields.io/github/stars/rongyaofang/GoT)](https://github.com/rongyaofang/GoT)
[![Dataset](https://img.shields.io/badge/Dataset-Available-brightgreen?style=flat-square)](https://github.com/rongyaofang/GoT#released-datasets)
[![Model](https://img.shields.io/badge/Model-Available-orange?style=flat-square)](https://github.com/rongyaofang/GoT#released-model-got-framework)


- [Visual planning: Let's think only with images](https://arxiv.org/abs/2505.11409) (Mar., 2025) <br>
[![GitHub stars](https://img.shields.io/github/stars/yix8/VisualPlanning)](https://github.com/yix8/VisualPlanning)








## üéì Reasoning with prompt

Tailoring pre-trained models for visual reasoning through targeted fine-tuning on specialized datasets. This approach leverages instruction-following data and demonstrations of reasoning steps to enhance model capabilities.

- [Generating images with multimodal language models](https://arxiv.org/abs/2305.17216) ![](https://img.shields.io/badge/abs-2023.05-red)

---

## üèÜ RL for Self-Evolution

Harnessing the power of Reinforcement Learning to teach models how to reason with images through trial, error, and reward. These approaches enable agents to learn complex visual behaviors, tool interactions, and even intrinsic motivation for exploration.

- [Ground-R1: Incentivizing Grounded Visual Reasoning via Reinforcement Learning](https://arxiv.org/abs/2505.20272) ![](https://img.shields.io/badge/abs-2025.05-red)

---

## üìö Benchmarks
*Essential resources for understanding the broader landscape and evaluating progress in visual reasoning.*

- [A Cognitive Evaluation Benchmark of Image Reasoning and Description for Large Vision-Language Models](https://arxiv.org/abs/2402.18409) ![](https://img.shields.io/badge/abs-2024.02-red)


## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=exped1230/Awesome_Image_Generation_with_Thinking&type=Date)](https://www.star-history.com/#exped1230/Awesome_Image_Generation_with_Thinking&Date)



